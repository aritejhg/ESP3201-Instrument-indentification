{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05760cd4-cebb-43d6-afa8-2fcb73c5cc7f",
   "metadata": {},
   "source": [
    "# Training vgg11 on gammatone spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb851847-3977-4565-af1e-4569015dc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ..\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467088a4-97dc-46fc-96e7-606be2b5c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%cd openmic-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102af952-82e4-46ae-b2d6-ee33165a43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab249972-aacf-4b8c-aaee-db33aa207930",
   "metadata": {},
   "source": [
    "## data loading and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73f1e6-89d8-4a7f-a9a6-876fe7b93af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"openmic-2018.npz\", allow_pickle = True)\n",
    "X, Y_true, Y_mask, sample_key = data['X'], data['Y_true'], data['Y_mask'], data['sample_key']\n",
    "print(X.shape, Y_true.shape, Y_mask.shape, sample_key.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8dc369-f403-4459-9030-68a93866ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_true[0])\n",
    "Y_true1 = np.where(Y_true > 0.5, 1, 0)\n",
    "print(Y_true1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e93b5a-78b1-45de-96fb-72d24143fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model\n",
    "CHANNELS = 3 # Keep RGB color channels to match the input format of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97fcf8-e8cd-45ff-b9bf-3dcbff2f74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cb30c-666b-4b18-b36b-aa973876fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n",
    "SHUFFLE_BUFFER_SIZE = 128 # Shuffle the training data by a chunck of 128 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf393c-9e27-43e4-ae49-1a0b9a3299f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # if is_training == True:\n",
    "    #     # This is a small dataset, only load it once, and keep it in memory.\n",
    "    #     dataset = dataset.cache()\n",
    "    # Shuffle the data each buffer size\n",
    "    # dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        \n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696c451-8abd-4ff9-889d-7e11daa278c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training split csv file\n",
    "with open(\"partitions/split01_train.csv\") as f:\n",
    "    train_IDs = f.readlines()\n",
    "    train_IDs = np.array([ID.strip() for ID in train_IDs])\n",
    "print(train_IDs.shape)\n",
    "\n",
    "# Load test split csv file\n",
    "with open(\"partitions/split01_test.csv\") as f:\n",
    "    test_IDs = f.readlines()\n",
    "    test_IDs = np.array([ID.strip() for ID in test_IDs])\n",
    "print(test_IDs.shape)\n",
    "\n",
    "# Get the training and testing split data into np arrays\n",
    "# Creates an array, which contains the index of the sample in data.  \n",
    "train_index = np.array([i for i in range(20000) if sample_key[i] in train_IDs])\n",
    "test_index = np.array([i for i in range(20000) if sample_key[i] in test_IDs])\n",
    "\n",
    "print(train_index.shape)\n",
    "print(test_index[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a811fc-d8e4-49ab-b502-ee2a37b8d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [f\"/home/studio-lab-user/sagemaker-studiolab-notebooks/ESP3201-Instrument-indentification/openmic-2018/audio-gammatone/{i[:3]}/{i}.png\" for i in train_IDs]\n",
    "print(train_paths[1])\n",
    "\n",
    "test_paths = [f\"/home/studio-lab-user/sagemaker-studiolab-notebooks/ESP3201-Instrument-indentification/openmic-2018/audio-gammatone/{i[:3]}/{i}.png\" for i in test_IDs]\n",
    "print(test_paths[1])\n",
    "\n",
    "train_labels = [Y_true1[i] for i in train_index]\n",
    "print(train_labels[1])\n",
    "\n",
    "test_labels = [Y_true1[i] for i in test_index]\n",
    "print(test_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf00fd-00d5-46bb-9c94-c405c965dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = create_dataset(train_paths, train_labels)\n",
    "print(x_train)\n",
    "\n",
    "x_test = create_dataset(test_paths, test_labels)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a49fb5-3cbb-4341-94b8-63d770bac94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the dataloader\n",
    "for element in x_train.as_numpy_iterator():\n",
    "    print(element[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cc7c8-3294-49b1-8031-d253f4cda6c5",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7030e9-ff37-426a-b76c-9003487bc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary layers  \n",
    "from tensorflow.keras.layers import Input, Conv2D \n",
    "from tensorflow.keras.layers import MaxPool2D, Flatten, Dense \n",
    "from tensorflow.keras import Model\n",
    "# input\n",
    "\n",
    "input = Input(shape =(224,224,3))\n",
    "# 1st Conv Block\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(input)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# 2nd Conv Block\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# 3rd Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# 4th Conv block\n",
    "x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Flatten()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "output = Dense(units = 20, activation ='sigmoid')(x)\n",
    "\n",
    "# creating the model\n",
    "\n",
    "model = Model (inputs=input, outputs =output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a4089-e216-4ea1-886e-5957fdbcf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 40\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af41721-54f0-4c11-8a21-e2bd98bc5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=loss_fn, metrics = ['Recall', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74e63b-ca43-44d9-b92a-8494c0eb1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "# train\n",
    "history = model.fit(x_train, epochs = EPOCHS, callbacks=[earlyStopping, reduce_lr_loss], validation_data = x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565e332-9c78-4c56-a88d-033130290a6a",
   "metadata": {},
   "source": [
    "## visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895857d-7f66-49b0-a4fd-e0cc3785c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cc8db-327f-4197-8a5d-654a8f2dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b93e3-9afd-4b50-abf1-a9c8fe8c0a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size = 1)\n",
    "print(y_pred[0])\n",
    "y_pred1 = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5b6a7-7996-4eed-8222-68f292c770dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(multilabel_confusion_matrix(test_labels, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372bd5-7ed4-49ec-be32-4d7a95d37e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_pred1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
