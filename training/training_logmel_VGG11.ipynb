{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb851847-3977-4565-af1e-4569015dc0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467088a4-97dc-46fc-96e7-606be2b5c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/openmic-2018\n"
     ]
    }
   ],
   "source": [
    "cd openmic-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23cf30-7326-4ad6-b3e7-6444c37d1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45c529ed-d1e0-4131-820b-451356b82110",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102af952-82e4-46ae-b2d6-ee33165a43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b73f1e6-89d8-4a7f-a9a6-876fe7b93af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10, 128) (20000, 20) (20000, 20) (20000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"openmic-2018.npz\", allow_pickle = True)\n",
    "X, Y_true, Y_mask, sample_key = data['X'], data['Y_true'], data['Y_mask'], data['sample_key']\n",
    "print(X.shape, Y_true.shape, Y_mask.shape, sample_key.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8dc369-f403-4459-9030-68a93866ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5     0.5     0.5     0.5     0.17105 0.5     0.5     0.      0.5\n",
      " 0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.      0.5\n",
      " 0.5     0.5    ]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_true[0])\n",
    "Y_true1 = np.where(Y_true > 0.5, 1, 0)\n",
    "print(Y_true1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e93b5a-78b1-45de-96fb-72d24143fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model\n",
    "CHANNELS = 3 # Keep RGB color channels to match the input format of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb97fcf8-e8cd-45ff-b9bf-3dcbff2f74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f81cb30c-666b-4b18-b36b-aa973876fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n",
    "SHUFFLE_BUFFER_SIZE = 128 # Shuffle the training data by a chunck of 128 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fcf393c-9e27-43e4-ae49-1a0b9a3299f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # if is_training == True:\n",
    "    #     # This is a small dataset, only load it once, and keep it in memory.\n",
    "    #     dataset = dataset.cache()\n",
    "    # Shuffle the data each buffer size\n",
    "    # dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        \n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3696c451-8abd-4ff9-889d-7e11daa278c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14915,)\n",
      "(5085,)\n",
      "(14915,)\n",
      "[29 30 31 32 33 34 35 36 37]\n"
     ]
    }
   ],
   "source": [
    "# PROLLY DONT NEED THIS\n",
    "# Load training split csv file\n",
    "with open(\"partitions/split01_train.csv\") as f:\n",
    "    train_IDs = f.readlines()\n",
    "    train_IDs = np.array([ID.strip() for ID in train_IDs])\n",
    "print(train_IDs.shape)\n",
    "\n",
    "# Load test split csv file\n",
    "with open(\"partitions/split01_test.csv\") as f:\n",
    "    test_IDs = f.readlines()\n",
    "    test_IDs = np.array([ID.strip() for ID in test_IDs])\n",
    "print(test_IDs.shape)\n",
    "\n",
    "# Get the training and testing split data into np arrays\n",
    "# Creates an array, which contains the index of the sample in data.  \n",
    "train_index = np.array([i for i in range(20000) if sample_key[i] in train_IDs])\n",
    "test_index = np.array([i for i in range(20000) if sample_key[i] in test_IDs])\n",
    "\n",
    "print(train_index.shape)\n",
    "print(test_index[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a811fc-d8e4-49ab-b502-ee2a37b8d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/openmic-2018/audio-logmel/000/000135_483840.png\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/openmic-2018/audio-logmel/000/000308_61440.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_paths = [f\"/home/studio-lab-user/sagemaker-studiolab-notebooks/openmic-2018/audio-logmel/{i[:3]}/{i}.png\" for i in train_IDs]\n",
    "print(train_paths[1])\n",
    "\n",
    "test_paths = [f\"/home/studio-lab-user/sagemaker-studiolab-notebooks/openmic-2018/audio-logmel/{i[:3]}/{i}.png\" for i in test_IDs]\n",
    "print(test_paths[1])\n",
    "\n",
    "train_labels = [Y_true1[i] for i in train_index]\n",
    "print(train_labels[1])\n",
    "\n",
    "test_labels = [Y_true1[i] for i in test_index]\n",
    "print(test_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70bf00fd-00d5-46bb-9c94-c405c965dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))>\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "x_train = create_dataset(train_paths, train_labels)\n",
    "print(x_train)\n",
    "\n",
    "x_test = create_dataset(test_paths, test_labels)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a49fb5-3cbb-4341-94b8-63d770bac94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20)\n"
     ]
    }
   ],
   "source": [
    "for element in x_train.as_numpy_iterator():\n",
    "    print(element[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7030e9-ff37-426a-b76c-9003487bc746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100352)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              411045888 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                81940     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 432,410,004\n",
      "Trainable params: 432,410,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import necessary layers  \n",
    "from tensorflow.keras.layers import Input, Conv2D \n",
    "from tensorflow.keras.layers import MaxPool2D, Flatten, Dense \n",
    "from tensorflow.keras import Model\n",
    "# input\n",
    "\n",
    "input = Input(shape =(224,224,3))\n",
    "# 1st Conv Block\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(input)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# 2nd Conv Block\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# 3rd Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# 4th Conv block\n",
    "x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Flatten()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "output = Dense(units = 20, activation ='sigmoid')(x)\n",
    "\n",
    "# creating the model\n",
    "\n",
    "model = Model (inputs=input, outputs =output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87a4089-e216-4ea1-886e-5957fdbcf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 50\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af41721-54f0-4c11-8a21-e2bd98bc5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=loss_fn, metrics = ['Recall', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74e63b-ca43-44d9-b92a-8494c0eb1e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/50\n",
      "243/467 [==============>...............] - ETA: 54s - loss: 0.2055 - recall: 0.0034 - precision: 0.0480"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "history = model.fit(x_train, epochs = EPOCHS, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data = x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895857d-7f66-49b0-a4fd-e0cc3785c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cc8db-327f-4197-8a5d-654a8f2dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['recall'])\n",
    "# plt.plot(history.history['precision'])\n",
    "# plt.title('train precision-recall')\n",
    "# plt.ylabel('precision')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b93e3-9afd-4b50-abf1-a9c8fe8c0a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size = 1)\n",
    "print(y_pred[0])\n",
    "y_pred1 = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5b6a7-7996-4eed-8222-68f292c770dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(multilabel_confusion_matrix(test_labels, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372bd5-7ed4-49ec-be32-4d7a95d37e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cf6ad-c902-4762-929b-96bd0fb39a92",
   "metadata": {},
   "source": [
    "# Load model and get classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab27990-2619-4f30-be59-09bbe43922b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3ebfd5-89a9-49e8-a1b4-09e3b88d9771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/openmic-2018\n"
     ]
    }
   ],
   "source": [
    "cd openmic-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd894b2-6b43-40b4-814d-b2045d199678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 07:53:16.579832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-02 07:53:16.581416: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-02 07:53:16.581693: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n",
      "2022-11-02 07:53:16.583731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-02 07:53:17.130698: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n",
      "2022-11-02 07:53:19.084774: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n",
      "2022-11-02 07:53:19.618754: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n",
      "2022-11-02 07:53:39.834592: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n",
      "2022-11-02 07:53:42.775906: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('mdl_wts.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e87cf97-ac7a-47c2-99db-339b260d5b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 454s 3s/step\n",
      "[1.49278832e-03 1.74067020e-02 2.78160349e-03 4.45976183e-02\n",
      " 4.81306226e-04 2.84352642e-03 1.22244805e-02 1.28953718e-03\n",
      " 4.77167685e-03 6.69724960e-03 1.91715162e-03 6.23594131e-03\n",
      " 1.60754938e-03 7.38076586e-03 1.50279123e-02 5.80802793e-03\n",
      " 1.66151091e-03 3.06834909e-03 6.45433087e-03 5.26214898e-01]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, batch_size = 1)\n",
    "print(y_pred[0])\n",
    "y_pred1 = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30a41d84-6472-4b42-ba7e-1e2dab524c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.08      0.13       115\n",
      "           1       0.24      0.04      0.06       140\n",
      "           2       0.43      0.15      0.22       134\n",
      "           3       0.47      0.27      0.34       226\n",
      "           4       0.40      0.01      0.03       137\n",
      "           5       0.64      0.36      0.46       297\n",
      "           6       0.62      0.39      0.48       278\n",
      "           7       0.45      0.14      0.22       175\n",
      "           8       0.63      0.17      0.27       286\n",
      "           9       0.40      0.01      0.02       211\n",
      "          10       0.51      0.10      0.17       193\n",
      "          11       0.00      0.00      0.00       121\n",
      "          12       0.86      0.27      0.41       285\n",
      "          13       0.66      0.12      0.20       305\n",
      "          14       0.57      0.25      0.34       268\n",
      "          15       0.00      0.00      0.00       228\n",
      "          16       0.50      0.01      0.01       318\n",
      "          17       0.64      0.24      0.35       182\n",
      "          18       0.60      0.42      0.49       394\n",
      "          19       0.80      0.16      0.27       224\n",
      "\n",
      "   micro avg       0.60      0.18      0.28      4517\n",
      "   macro avg       0.49      0.16      0.22      4517\n",
      "weighted avg       0.53      0.18      0.25      4517\n",
      " samples avg       0.15      0.14      0.14      4517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eca972-74c3-4815-acf1-7b674c5ab8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
